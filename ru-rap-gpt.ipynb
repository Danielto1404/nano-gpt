{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from tokenizer import CharTokenizer\n",
    "from model import GPT\n",
    "from model.config import GPTConfig, config_for"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Любви, надежды, тихой славы\n",
      " Недолго нежил нас обман,\n",
      " Исчезли юные забавы,\n",
      " Как сон, как утренний туман;\n",
      " Но в нас горит еще желанье,\n",
      " Под гнетом власти роковой\n",
      " Нетерпеливою душой\n",
      " Отчизны внемлем призыванье.\n",
      " Мы ждем с томленьем упованья\n",
      " Минуты вольности святой,\n",
      " Как ждет любовник молодой\n",
      " Минуты верного свиданья.\n",
      " Пока свободою горим,\n",
      " Пока сердца для чести живы,\n",
      " Мой друг, отчизне посвятим\n",
      " Души прекрасные порывы!\n",
      " Товарищ, верь: взойдет она,\n",
      " Звезда пленительного счастья,\n",
      " Россия вспрянет ото сна,\n",
      " И на обломках самовластья\n",
      " Напишут наши имена!\n"
     ]
    }
   ],
   "source": [
    "with open(\"resources/pushkin.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "text = \" \".join(lines)\n",
    "print(text)\n",
    "\n",
    "tokenizer = CharTokenizer.train([text])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "GPTConfig(vocab_size=36, num_decoder_layers=4, embedding_dim=384, dim_feedforward=768, num_decoder_heads=6, decoder_head_dim=64, max_seq_len=256, dropout=0.5)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = config_for(\"tiny\", vocab_size=len(tokenizer.vocab), dropout=0.2)\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.vocab_length(),\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=768,\n",
    "    embedding_dim=384,\n",
    "    num_decoder_heads=6,\n",
    "    decoder_head_dim=64,\n",
    "    dropout=0.5,\n",
    "    max_seq_len=256\n",
    ")\n",
    "\n",
    "config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tokenizer.save(\"fuck.tokenizer.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT(\n  (tok_embeds): Embedding(36, 384)\n  (decoder): GPTDecoder(\n    (decoders): ModuleList(\n      (0): GPTDecoderBlock(\n        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (mha): MultiHeadAttention(\n          (heads): ModuleList(\n            (0): AttentionHead(head_dim=64)\n            (1): AttentionHead(head_dim=64)\n            (2): AttentionHead(head_dim=64)\n            (3): AttentionHead(head_dim=64)\n            (4): AttentionHead(head_dim=64)\n            (5): AttentionHead(head_dim=64)\n          )\n          (projection): Sequential(\n            (0): Linear(in_features=384, out_features=384, bias=True)\n            (1): Dropout(p=0.5, inplace=False)\n          )\n        )\n        (mlp): FeedForward(\n          (ff): Sequential(\n            (0): Linear(in_features=384, out_features=768, bias=True)\n            (1): GoogleGELU()\n            (2): Linear(in_features=768, out_features=384, bias=True)\n            (3): Dropout(p=0.5, inplace=False)\n          )\n        )\n      )\n      (1): GPTDecoderBlock(\n        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (mha): MultiHeadAttention(\n          (heads): ModuleList(\n            (0): AttentionHead(head_dim=64)\n            (1): AttentionHead(head_dim=64)\n            (2): AttentionHead(head_dim=64)\n            (3): AttentionHead(head_dim=64)\n            (4): AttentionHead(head_dim=64)\n            (5): AttentionHead(head_dim=64)\n          )\n          (projection): Sequential(\n            (0): Linear(in_features=384, out_features=384, bias=True)\n            (1): Dropout(p=0.5, inplace=False)\n          )\n        )\n        (mlp): FeedForward(\n          (ff): Sequential(\n            (0): Linear(in_features=384, out_features=768, bias=True)\n            (1): GoogleGELU()\n            (2): Linear(in_features=768, out_features=384, bias=True)\n            (3): Dropout(p=0.5, inplace=False)\n          )\n        )\n      )\n      (2): GPTDecoderBlock(\n        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (mha): MultiHeadAttention(\n          (heads): ModuleList(\n            (0): AttentionHead(head_dim=64)\n            (1): AttentionHead(head_dim=64)\n            (2): AttentionHead(head_dim=64)\n            (3): AttentionHead(head_dim=64)\n            (4): AttentionHead(head_dim=64)\n            (5): AttentionHead(head_dim=64)\n          )\n          (projection): Sequential(\n            (0): Linear(in_features=384, out_features=384, bias=True)\n            (1): Dropout(p=0.5, inplace=False)\n          )\n        )\n        (mlp): FeedForward(\n          (ff): Sequential(\n            (0): Linear(in_features=384, out_features=768, bias=True)\n            (1): GoogleGELU()\n            (2): Linear(in_features=768, out_features=384, bias=True)\n            (3): Dropout(p=0.5, inplace=False)\n          )\n        )\n      )\n      (3): GPTDecoderBlock(\n        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (mha): MultiHeadAttention(\n          (heads): ModuleList(\n            (0): AttentionHead(head_dim=64)\n            (1): AttentionHead(head_dim=64)\n            (2): AttentionHead(head_dim=64)\n            (3): AttentionHead(head_dim=64)\n            (4): AttentionHead(head_dim=64)\n            (5): AttentionHead(head_dim=64)\n          )\n          (projection): Sequential(\n            (0): Linear(in_features=384, out_features=384, bias=True)\n            (1): Dropout(p=0.5, inplace=False)\n          )\n        )\n        (mlp): FeedForward(\n          (ff): Sequential(\n            (0): Linear(in_features=384, out_features=768, bias=True)\n            (1): GoogleGELU()\n            (2): Linear(in_features=768, out_features=384, bias=True)\n            (3): Dropout(p=0.5, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): GPTClassifierHead(\n    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    (drop): Dropout(p=0.5, inplace=False)\n    (clf): Linear(in_features=384, out_features=36, bias=True)\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPT(config)\n",
    "gpt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "557"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(text)\n",
    "len(encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BATCHES = len(encoded) // BATCH_SIZE\n",
    "EPOCHS = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 3.8048110008239746\n",
      "epoch 2 loss: 2.9936805963516235\n",
      "epoch 3 loss: 2.6144509315490723\n",
      "epoch 4 loss: 2.3087966442108154\n",
      "epoch 5 loss: 2.018452823162079\n",
      "epoch 6 loss: 1.7976194620132446\n",
      "epoch 7 loss: 1.6444581747055054\n",
      "epoch 8 loss: 1.4550820589065552\n",
      "epoch 9 loss: 1.2621997594833374\n",
      "epoch 10 loss: 1.0676565170288086\n",
      "epoch 11 loss: 0.8886569738388062\n",
      "epoch 12 loss: 0.7775632441043854\n",
      "epoch 13 loss: 0.5741545557975769\n",
      "epoch 14 loss: 0.47391851246356964\n",
      "epoch 15 loss: 0.38171617686748505\n",
      "epoch 16 loss: 0.3044995069503784\n",
      "epoch 17 loss: 0.2503356635570526\n",
      "epoch 18 loss: 0.19674308598041534\n",
      "epoch 19 loss: 0.16712970286607742\n",
      "epoch 20 loss: 0.14521539211273193\n",
      "epoch 21 loss: 0.1163438968360424\n",
      "epoch 22 loss: 0.1030660904943943\n",
      "epoch 23 loss: 0.08055250719189644\n",
      "epoch 24 loss: 0.07905955240130424\n",
      "epoch 25 loss: 0.06037338450551033\n",
      "epoch 26 loss: 0.06072022579610348\n",
      "epoch 27 loss: 0.045732857659459114\n",
      "epoch 28 loss: 0.05219606123864651\n",
      "epoch 29 loss: 0.04405752569437027\n",
      "epoch 30 loss: 0.03346303850412369\n",
      "epoch 31 loss: 0.03296651877462864\n",
      "epoch 32 loss: 0.03082391992211342\n",
      "epoch 33 loss: 0.023090139031410217\n",
      "epoch 34 loss: 0.023448767140507698\n",
      "epoch 35 loss: 0.025057599879801273\n",
      "epoch 36 loss: 0.01832447201013565\n",
      "epoch 37 loss: 0.016997072845697403\n",
      "epoch 38 loss: 0.020227018743753433\n",
      "epoch 39 loss: 0.01369419600814581\n",
      "epoch 40 loss: 0.022115833591669798\n",
      "epoch 41 loss: 0.018946080934256315\n",
      "epoch 42 loss: 0.016449678223580122\n",
      "epoch 43 loss: 0.01253051171079278\n",
      "epoch 44 loss: 0.012749714311212301\n",
      "epoch 45 loss: 0.013800663407891989\n",
      "epoch 46 loss: 0.010643266141414642\n",
      "epoch 47 loss: 0.010101840365678072\n",
      "epoch 48 loss: 0.010828928556293249\n",
      "epoch 49 loss: 0.014012944884598255\n",
      "epoch 50 loss: 0.008628467097878456\n",
      "epoch 51 loss: 0.008625636110082269\n",
      "epoch 52 loss: 0.011500116437673569\n",
      "epoch 53 loss: 0.0079829057212919\n",
      "epoch 54 loss: 0.011313878931105137\n",
      "epoch 55 loss: 0.005897460039705038\n",
      "epoch 56 loss: 0.015469012781977654\n",
      "epoch 57 loss: 0.006883974187076092\n",
      "epoch 58 loss: 0.008388174464926124\n",
      "epoch 59 loss: 0.010533586144447327\n",
      "epoch 60 loss: 0.00743201095610857\n",
      "epoch 61 loss: 0.005565247731283307\n",
      "epoch 62 loss: 0.006129165645688772\n",
      "epoch 63 loss: 0.0070583990309387445\n",
      "epoch 64 loss: 0.0064603835344314575\n",
      "epoch 65 loss: 0.0073142205365002155\n",
      "epoch 66 loss: 0.005703732604160905\n",
      "epoch 67 loss: 0.006330985343083739\n",
      "epoch 68 loss: 0.007180325221270323\n",
      "epoch 69 loss: 0.005303391255438328\n",
      "epoch 70 loss: 0.005165452836081386\n",
      "epoch 71 loss: 0.0054183658212423325\n",
      "epoch 72 loss: 0.005324047990143299\n",
      "epoch 73 loss: 0.005295999813824892\n",
      "epoch 74 loss: 0.004534335806965828\n",
      "epoch 75 loss: 0.003871943918056786\n",
      "epoch 76 loss: 0.004142051562666893\n",
      "epoch 77 loss: 0.00390740402508527\n",
      "epoch 78 loss: 0.0038057995261624455\n",
      "epoch 79 loss: 0.00426365016028285\n",
      "epoch 80 loss: 0.004188769031316042\n",
      "epoch 81 loss: 0.0037638379726558924\n",
      "epoch 82 loss: 0.0033691019052639604\n",
      "epoch 83 loss: 0.004098267527297139\n",
      "epoch 84 loss: 0.0033780665835365653\n",
      "epoch 85 loss: 0.0033489303896203637\n",
      "epoch 86 loss: 0.0034538713516667485\n",
      "epoch 87 loss: 0.00294259877409786\n",
      "epoch 88 loss: 0.00278742634691298\n",
      "epoch 89 loss: 0.003252729889936745\n",
      "epoch 90 loss: 0.002800450543873012\n",
      "epoch 91 loss: 0.0029363533249124885\n",
      "epoch 92 loss: 0.003161280881613493\n",
      "epoch 93 loss: 0.002963214530609548\n",
      "epoch 94 loss: 0.002636585384607315\n",
      "epoch 95 loss: 0.0023839459754526615\n",
      "epoch 96 loss: 0.003200766979716718\n",
      "epoch 97 loss: 0.0025321224238723516\n",
      "epoch 98 loss: 0.0023442100500687957\n",
      "epoch 99 loss: 0.0024792461190372705\n",
      "epoch 100 loss: 0.002737068571150303\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i in range(BATCHES):\n",
    "        optimizer.zero_grad()\n",
    "        x = encoded[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "        y = encoded[i * BATCH_SIZE + 1: (i + 1) * BATCH_SIZE + 1]\n",
    "\n",
    "        x = torch.LongTensor(x).unsqueeze(0)\n",
    "        y = torch.LongTensor(y).unsqueeze(0)\n",
    "        m = torch.tril(torch.ones(BATCH_SIZE, BATCH_SIZE))\n",
    "\n",
    "        output = gpt(x, attention_mask=m, output_attentions=True)\n",
    "\n",
    "        loss = criterion(output.logits.transpose(1, 2), y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {e + 1} loss: {epoch_loss / BATCHES}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "исчезли сдежды, тихой славы\n",
      " недолго нежил нас обман,\n",
      " исчезли юные забавы,\n",
      " как сон, как утренний туман;\n",
      " н\n"
     ]
    }
   ],
   "source": [
    "text = \"исчезли \"\n",
    "\n",
    "iters = 100\n",
    "\n",
    "for _ in range(iters):\n",
    "    input_ids = torch.tensor(tokenizer.encode([text[-BATCH_SIZE:]]))\n",
    "    with torch.no_grad():\n",
    "        gpt.eval()\n",
    "        logits = gpt(input_ids).logits\n",
    "        logits = logits[0, -1]\n",
    "\n",
    "        token_id = logits.argmax().item()\n",
    "\n",
    "        text += tokenizer.id_to_token(token_id)\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
